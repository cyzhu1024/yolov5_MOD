# YOLOv5 ğŸš€ by Ultralytics, GPL-3.0 license
"""
Loss functions
"""

import torch
import torch.nn as nn

from utils.metrics import bbox_iou
from utils.torch_utils import de_parallel


def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441
    # return positive, negative label smoothing BCE targets
    return 1.0 - 0.5 * eps, 0.5 * eps


class BCEBlurWithLogitsLoss(nn.Module):
    # BCEwithLogitLoss() with reduced missing label effects.
    def __init__(self, alpha=0.05):
        super().__init__()
        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()
        self.alpha = alpha

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        pred = torch.sigmoid(pred)  # prob from logits
        dx = pred - true  # reduce only missing label effects
        # dx = (pred - true).abs()  # reduce missing label and false label effects
        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))
        loss *= alpha_factor
        return loss.mean()


class FocalLoss(nn.Module):
    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        # p_t = torch.exp(-loss)
        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability

        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py
        pred_prob = torch.sigmoid(pred)  # prob from logits
        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = (1.0 - p_t) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class QFocalLoss(nn.Module):
    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)

        pred_prob = torch.sigmoid(pred)  # prob from logits
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = torch.abs(true - pred_prob) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class ComputeLoss:
    sort_obj_iou = False

    # Compute losses
    def __init__(self, model, autobalance=False):
        device = next(model.parameters()).device  # get model device
        h = model.hyp  # hyperparameters

        # Define criteria äºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°
        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device))
        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device))

        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3
        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets

        # Focal loss
        g = h['fl_gamma']  # focal loss gamma
        if g > 0:
            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)

        m = de_parallel(model).model[-1]  # Detect() module
        self.balance = {3: [4.0, 1.0, 0.4]}.get(m.nl, [4.0, 1.0, 0.25, 0.06, 0.02])  # P3-P7
        self.ssi = list(m.stride).index(16) if autobalance else 0  # stride 16 index
        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, h, autobalance
        self.na = m.na  # number of anchors
        self.nc = m.nc  # number of classes
        self.nl = m.nl  # number of layers
        self.anchors = m.anchors
        self.device = device

    def __call__(self, p, targets):  # predictions, targets
        lcls = torch.zeros(1, device=self.device)  # class loss
        lbox = torch.zeros(1, device=self.device)  # box loss
        lobj = torch.zeros(1, device=self.device)  # object loss
        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets

        # Losses
        for i, pi in enumerate(p):  # layer index, layer predictions
            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx
            tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj

            n = b.shape[0]  # number of targets
            if n: # a, b, gi, gjçš„shapeå‡æ˜¯(156), p[0]çš„shape(4,3,80,80,85)
                # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0
                pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions, ç­›å‡ºmaskæ©ç ä¸ºtrueçš„æ ¼å­
                # pxy, pwhçš„shapeå‡æ˜¯(156,2), pclsçš„shapeæ˜¯(156,80), _çš„shape(156,1)

                # Regression è®¡ç®—CIOU
                pxy = pxy.sigmoid() * 2 - 0.5
                pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i] # anchors[0]çš„shape(156,2), è¿™é‡Œä¸ºä»€ä¹ˆæ˜¯(0,4)çš„åŸå› å¯èƒ½ä¸è¶…å‚æ•°anchor_tæœ‰å…³
                pbox = torch.cat((pxy, pwh), 1)  # predicted box ä¸‹é¢çš„squeeze()èƒ½å¤Ÿé™¤å»æ•°å€¼ä¸º1çš„ç»´åº¦
                iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target) iouçš„shape(156) è¿™é‡Œçš„pboxä¸tboxéƒ½æ˜¯åœ¨å°æ ¼å­ä¸Šå½’ä¸€åŒ–åçš„æ•°æ®
                lbox += (1.0 - iou).mean()  # iou loss

                # Objectness è®¡ç®—æœ‰ç‰©ä½“ç½®ä¿¡åº¦
                iou = iou.detach().clamp(0).type(tobj.dtype) # detach()ç”¨äºä»å½“å‰è®¡ç®—å›¾ä¸­åˆ†ç¦»å¼ é‡ã€‚å®ƒè¿”å›ä¸€ä¸ªä¸éœ€è¦æ¢¯åº¦çš„æ–°å¼ é‡
                if self.sort_obj_iou: # detach()ç”¨äºé˜»æ–­æ¢¯åº¦åå‘ä¼ æ’­, é»˜è®¤False
                    j = iou.argsort() # argsort()ç”¨äºè¿”å›ä»å°åˆ°å¤§æ’åºçš„åæ ‡
                    b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]
                if self.gr < 1: # é»˜è®¤ä¸º1.0
                    iou = (1.0 - self.gr) + self.gr * iou
                tobj[b, a, gj, gi] = iou  # iou ratio tobjçš„shape(4,3,80,80)
                # iouä¸­ä¸ä¸º0çš„æ•°æœ‰155ä¸ªï¼Œä½†æ˜¯tobjä¸­åªæœ‰151ä¸ªï¼Œå¯èƒ½æœ‰äº›æ•°æ®ä¸‹æ ‡é‡åˆäº†

                # Classification è®¡ç®—åˆ†ç±»æ¦‚ç‡
                if self.nc > 1:  # cls loss (only if multiple classes) torch.full_like(shape, val)ä¼šç”Ÿæˆä¸€ä¸ªå€¼å…¨ä¸ºvalï¼Œå½¢çŠ¶ä¸ºshapeçš„tensor
                    t = torch.full_like(pcls, self.cn, device=self.device)  # targets self.cn=0
                    t[range(n), tcls[i]] = self.cp # range(n)å’Œtcls[0]çš„shapeéƒ½æ˜¯(156), self.cp=1ä»…maskæ©ç ä¸ºtrueçš„é¢„æµ‹æ¡†éœ€è¦è®¡ç®—åˆ†ç±»æŸå¤±
                    lcls += self.BCEcls(pcls, t)  # BCE

                # Append targets to text file
                # with open('targets.txt', 'a') as file:
                #     [file.write('%11.5g ' * 4 % tuple(x) + '\n') for x in torch.cat((txy[i], twh[i]), 1)]

            obji = self.BCEobj(pi[..., 4], tobj) # pi[..., 4]å¾—åˆ°çš„shapeä¸º(4,3,80,80,1)
            lobj += obji * self.balance[i]  # obj loss
            if self.autobalance: # é»˜è®¤ä¸ºFalse
                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()

        if self.autobalance:
            self.balance = [x / self.balance[self.ssi] for x in self.balance]
        lbox *= self.hyp['box'] # 0.05 CIOU
        lobj *= self.hyp['obj'] # 1.0  ç½®ä¿¡åº¦
        lcls *= self.hyp['cls'] # 0.5  åˆ†ç±»æ¦‚ç‡
        bs = tobj.shape[0]  # batch size

        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach() # Tensor.detach() çš„ä½œç”¨æ˜¯é˜»æ–­åå‘æ¢¯åº¦ä¼ æ’­

    def build_targets(self, p, targets): # targetsçš„shape=[42, 6]
        # Build targets for compute_loss(), input targets(image,class,x,y,w,h)
        na, nt = self.na, targets.shape[0]  # number of anchors, targets
        tcls, tbox, indices, anch = [], [], [], [] # .repeat(2, 3), ä¼šæŠŠshapeä¸º[1, 2]çš„tensorå¤åˆ¶å˜ä¸ºshapeæ˜¯[2, 6]çš„tensor
        gain = torch.ones(7, device=self.device)  # normalized to gridspace gain .repeat()å‡½æ•°ä¼šåœ¨å¯¹åº”ç»´åº¦å¤åˆ¶tensorä¸ªæ•°
        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt)
        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # append anchor indicesï¼Œaiçš„shape=[3,42]
        # targetsçš„shape=[3,42,7], ç”±[3,42,6]+[3,42,1]æ‹¼æ¥è€Œæˆ
        # targetsçš„shape[0]è¡¨ç¤ºanchorä¸ªæ•°ï¼Œshape[1]è¡¨ç¤ºç›®æ ‡æ¡†ä¸ªæ•°ï¼Œshape[2]è¡¨ç¤ºæ•°æ®ä¸ªæ•°
        # å…¶ä¸­shape[2]ä¸º7ï¼Œ7ä¸ªæ•°æ®è§£é‡Šä¸º[img_idx, class, x, y, w, h, anchor_idx]
        g = 0.5  # bias
        off = torch.tensor(
            [
                [0, 0], # åŸæ¥çš„æ ¼å­ï¼Œä¿æŒä¸å˜
                [1, 0], # å·¦æ ¼å­ï¼Œround(x-0.5)
                [0, 1], # ä¸Šæ ¼å­ï¼Œround(y-0.5)
                [-1, 0], # å³æ ¼å­ï¼Œround(x+0.5)
                [0, -1],  # j,k,l,m ä¸‹æ ¼å­ï¼Œround(y+0.5)
                # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm
            ],
            device=self.device).float() * g  # offsets

        for i in range(self.nl): # anchorsçš„shapeä¸º[3,3,2] pæ˜¯predictionsçš„ç¼©å†™ï¼Œé‡Œé¢æœ‰ä¸‰ä¸ªtensor
            # p[0]shape(4,3,80,80,85)
            # p[1]shape(4,3,40,40,85)
            # p[2]shape(4,3,20,20,85)
            anchors, shape = self.anchors[i], p[i].shape # anchorsæ˜¯yamlæ–‡ä»¶ä¸­çš„anchorsä¸‰ä¸ªå…ƒç´ é™¤ä»¥8,16,32
            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # xyxy gain

            # Match targets to anchors è¿™é‡Œçš„targetsçš„shape=[3,42,7]
            t = targets * gain  # shape(3,n,7)
            if nt:
                # Matches è¿™ä¸€æ­¥æ˜¯æ±‚å®½é«˜æ¯”åœ¨1/4~4ä¹‹é—´çš„æ¡†
                r = t[..., 4:6] / anchors[:, None]  # wh ratio rçš„shape=[3,42,2], anchorsçš„shape=[3,2]
                j = torch.max(r, 1 / r).max(2)[0] < self.hyp['anchor_t']  # compare è¿™é‡Œçš„jå°±æ˜¯maskæ©ç 
                # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))
                sf = j.float().view(-1).sum() # é€šè¿‡sfå¾—å‡ºjä¸­Trueçš„ä¸ªæ•°ä¸º53ä¸ª
                t = t[j]  # filter tçš„shape(3,42,7)->(53,7), jçš„shapeä¸º(3,42), ä»¥ä¸€ä¸ªtensorä½œä¸ºç´¢å¼•å–å€¼å·¦ç«¯å¯¹é½

                # Offsets
                gxy = t[:, 2:4]  # grid xy gxyçš„shape=(53,2)
                gxi = gain[[2, 3]] - gxy  # inverse
                j, k = ((gxy % 1 < g) & (gxy > 1)).T # j, kçš„shapeéƒ½æ˜¯(53), æ­¤å¤„æ˜¯æ±‚æ˜¯å¦å–å¦å¤–ä¸¤ä¸ªæ¡†è¿›è¡Œè®¡ç®—
                l, m = ((gxi % 1 < g) & (gxi > 1)).T # .Tæ˜¯æ±‚tensorçš„è½¬ç½® ä¸‹é¢jçš„shapeæ˜¯(5, 53)
                j = torch.stack((torch.ones_like(j), j, k, l, m)) # å‡½æ•°stack()å¯¹åºåˆ—æ•°æ®å†…éƒ¨çš„å¼ é‡è¿›è¡Œæ‰©ç»´æ‹¼æ¥ï¼Œé»˜è®¤ç¬¬0ç»´
                t = t.repeat((5, 1, 1))[j] # ones_like()åŸºæœ¬åŠŸèƒ½æ˜¯æ ¹æ®ç»™å®šå¼ é‡ï¼Œç”Ÿæˆä¸å…¶å½¢çŠ¶ç›¸åŒçš„å…¨1å¼ é‡æˆ–å…¨0å¼ é‡,æ­¤å¤„tçš„shape(156,7)
                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j] # .repeat()å³ç«¯å¯¹é½,æ¯”å¦‚(2,3).repeat(3,1,2)->(3,2,6)
            else: # ä¸Šé¢çš„åŠ æ³•æ˜¯(1,53,2) + (5,1,2) -> (5,53,2)å¹¿æ’­æœºåˆ¶å³ç«¯å¯¹é½
                t = targets[0]
                offsets = 0

            # Define           .chunk(4, 1)è¡¨ç¤ºåœ¨ç¬¬1ç»´ä¸Šåˆ†ä¸º4ä»½, tçš„shapeä¸º(156,7),å› æ­¤ä¼šåˆ†ä¸º(156,2),(156,2),(156,2)å’Œ(156,1)
            bc, gxy, gwh, a = t.chunk(4, 1)  # (image, class), grid xy, grid wh, anchors
            a, (b, c) = a.long().view(-1), bc.long().T  # anchors, image, class
            gij = (gxy - offsets).long() # long()å‹æ•°æ®æ˜¯int64, offsetsçš„shape(156,2)
            gi, gj = gij.T  # grid indices gijçš„shapeæ˜¯(156,2), gi, gjåˆ†åˆ«ä¸ºå¦å¤–ä¸¤ä¸ªæ ¼å­çš„xå’Œyåæ ‡ï¼ˆå·¦ä¸Šè§’çš„åæ ‡ï¼Œæ‰€ä»¥éƒ½æ˜¯æ•´æ•°ï¼‰

            # Append                 clamp_()å‡½æ•°ç”¨äºé™å®štensoræ•°æ®çš„ä¸Šä¸‹ç•Œ
            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid
            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box æŠŠæ–°çš„boxçš„x,y,w,hæ•°æ®æ‹¼å¥½tboxé‡Œç¬¬ä¸€ä¸ªtupleä¸­tensorçš„shape(156,4)
            anch.append(anchors[a])  # anchors ä¸Šé¢gxy - gijæ¯ä¸ªå€¼çš„èŒƒå›´éƒ½æ˜¯(-0.5, 1.5)ï¼Œä¸Šé¢çš„æ“ä½œç›¸å½“äºåœ¨å¯¹åº”çš„å°æ ¼å­ä¸Šå½’ä¸€åŒ–ï¼Œgwhçš„æ•°æ®å¹¶ä¹Ÿæ˜¯å½’ä¸€åŒ–çš„
            tcls.append(c)  # class

        return tcls, tbox, indices, anch
